{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ad76c0",
   "metadata": {},
   "source": [
    "## Lab Assignment 2\n",
    "\n",
    "1. Nurul Alya binti Shuhami (SW01081037)\n",
    "2. Siti Nur Azleena binti Sabri (SW01081041)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f8645e",
   "metadata": {},
   "source": [
    "### Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a723493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Reviews.csv\", nrows=1000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c282f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63818f41",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c4d154",
   "metadata": {},
   "source": [
    "#### Remove punctuation, standardize, and removie digit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f791f501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffy at a great price there was a wide ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  i have bought several of the vitality canned d...  \n",
       "1  product arrived labeled as jumbo salted peanut...  \n",
       "2  this is a confection that has been around a fe...  \n",
       "3  if you are looking for the secret ingredient i...  \n",
       "4  great taffy at a great price there was a wide ...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import library\n",
    "import re\n",
    "import string\n",
    "\n",
    "#.strip() is to remove white spaces after you do substitution\n",
    "def get_cleaned_textdata(sentence):\n",
    "    modified_sentence = re.sub(r'<.*?>',' ', sentence)\n",
    "    modified_sentence = ''.join([i if i not in string.punctuation else ' ' for i in modified_sentence])\n",
    "    modified_sentence = re.sub(r'\\d+', ' ', modified_sentence)\n",
    "    modified_sentence = re.sub(r'\\s+', ' ', modified_sentence)\n",
    "    modified_sentence = re.sub(r'also', ' ', modified_sentence)\n",
    "    modified_sentence = modified_sentence.strip().lower()\n",
    "    return modified_sentence\n",
    "\n",
    "df['clean_text'] = df['Text'].apply(get_cleaned_textdata)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3e7c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select Text column\n",
    "data = df[['Score', 'clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c1702",
   "metadata": {},
   "source": [
    "#### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d43699b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [i, have, bought, several, of, the, vitality, ...\n",
      "1      [product, arrived, labeled, as, jumbo, salted,...\n",
      "2      [this, is, a, confection, that, has, been, aro...\n",
      "3      [if, you, are, looking, for, the, secret, ingr...\n",
      "4      [great, taffy, at, a, great, price, there, was...\n",
      "                             ...                        \n",
      "995    [black, market, hot, sauce, is, wonderful, my,...\n",
      "996    [man, what, can, i, say, this, salsa, is, the,...\n",
      "997    [this, sauce, is, so, good, with, just, about,...\n",
      "998    [not, hot, at, all, like, the, other, low, sta...\n",
      "999    [i, have, to, admit, i, was, a, sucker, for, t...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#import Natural language Processing (NLP) library called\n",
    "#Natural Language Toolkit (NLTK)\n",
    "import nltk\n",
    "\n",
    "#import the library for word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#the word tokens in the document\n",
    "data_token = df['clean_text'].apply(lambda x: word_tokenize(str(x)))\n",
    "print(data_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a39317e",
   "metadata": {},
   "source": [
    "#### Remove Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b90aa643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [bought, several, vitality, canned, dog, food,...\n",
      "1      [product, arrived, labeled, jumbo, salted, pea...\n",
      "2      [confection, around, centuries, light, pillowy...\n",
      "3      [looking, secret, ingredient, robitussin, beli...\n",
      "4      [great, taffy, great, price, wide, assortment,...\n",
      "                             ...                        \n",
      "995    [black, market, hot, sauce, wonderful, husband...\n",
      "996    [man, say, salsa, bomb, different, kinds, almo...\n",
      "997    [sauce, good, anything, like, adding, asian, f...\n",
      "998    [hot, like, low, star, reviewer, got, suckered...\n",
      "999    [admit, sucker, large, quantity, oz, shopping,...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#get the list of english stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#defining the function to remove stopwords from tokenized text\n",
    "def remove_stopwords(text):\n",
    "    output = []\n",
    "    for i in text:\n",
    "        if i not in stopwords:\n",
    "            output.append(i)\n",
    "    return output\n",
    "\n",
    "data_xstopwords = data_token.apply(remove_stopwords)\n",
    "print(data_xstopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786b291",
   "metadata": {},
   "source": [
    "#### Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9f24577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [bought, sever, vital, can, dog, food, product...\n",
      "1      [product, arriv, label, jumbo, salt, peanut, p...\n",
      "2      [confect, around, centuri, light, pillowi, cit...\n",
      "3      [look, secret, ingredi, robitussin, believ, fo...\n",
      "4      [great, taffi, great, price, wide, assort, yum...\n",
      "                             ...                        \n",
      "995    [black, market, hot, sauc, wonder, husband, lo...\n",
      "996    [man, say, salsa, bomb, differ, kind, almost, ...\n",
      "997    [sauc, good, anyth, like, ad, asian, food, any...\n",
      "998    [hot, like, low, star, review, got, sucker, se...\n",
      "999    [admit, sucker, larg, quantiti, oz, shop, hot,...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#import the Steming function from nltk library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "#defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = []\n",
    "    for word in text:\n",
    "        stemmed_word = porter_stemmer.stem(word)\n",
    "        stem_text.append(stemmed_word)\n",
    "    return stem_text\n",
    "\n",
    "data_porterstem = data_xstopwords.apply(stemming)\n",
    "print(data_porterstem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed6390",
   "metadata": {},
   "source": [
    "#### Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88a2417e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [bought, several, vitality, canned, dog, food,...\n",
      "1      [product, arrived, labeled, jumbo, salted, pea...\n",
      "2      [confection, around, century, light, pillowy, ...\n",
      "3      [looking, secret, ingredient, robitussin, beli...\n",
      "4      [great, taffy, great, price, wide, assortment,...\n",
      "                             ...                        \n",
      "995    [black, market, hot, sauce, wonderful, husband...\n",
      "996    [man, say, salsa, bomb, different, kind, almos...\n",
      "997    [sauce, good, anything, like, adding, asian, f...\n",
      "998    [hot, like, low, star, reviewer, got, suckered...\n",
      "999    [admit, sucker, large, quantity, oz, shopping,...\n",
      "Name: clean_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#importing the Lemmatizer function from nltk library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = []\n",
    "    for word in text:\n",
    "        lemmatized_word = wordnet_lemmatizer.lemmatize(word)\n",
    "        lemm_text.append(lemmatized_word)\n",
    "    return lemm_text\n",
    "\n",
    "clean_lemmatized = data_xstopwords.apply(lemmatizer)\n",
    "print(clean_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0536f344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Preprocessed_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "      <td>product arrived labeled jumbo salted peanut pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "      <td>confection around century light pillowy citrus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>great taffy at a great price there was a wide ...</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  i have bought several of the vitality canned d...   \n",
       "1  product arrived labeled as jumbo salted peanut...   \n",
       "2  this is a confection that has been around a fe...   \n",
       "3  if you are looking for the secret ingredient i...   \n",
       "4  great taffy at a great price there was a wide ...   \n",
       "\n",
       "                                   Preprocessed_Text  \n",
       "0  bought several vitality canned dog food produc...  \n",
       "1  product arrived labeled jumbo salted peanut pe...  \n",
       "2  confection around century light pillowy citrus...  \n",
       "3  looking secret ingredient robitussin believe f...  \n",
       "4  great taffy great price wide assortment yummy ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the tokens back into sentences\n",
    "df['Preprocessed_Text'] = clean_lemmatized.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "df.to_csv('preprocessed_amazon_reviews.csv', index=False)\n",
    "\n",
    "# Preview the preprocessed data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ebe852",
   "metadata": {},
   "source": [
    "### Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00761a2e",
   "metadata": {},
   "source": [
    "#### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4506dce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81c20514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(document):\n",
    "    word_count = Counter(document)\n",
    "    tf = {word: count/len(document) for word, count in word_count.items()}\n",
    "    return tf\n",
    "\n",
    "def compute_idf(documents):\n",
    "    N = len(documents)\n",
    "    idf = {}\n",
    "    all_words = set(word for doc in documents for word in doc)\n",
    "    for word in all_words:\n",
    "        count = sum(1 for doc in documents if word in doc)\n",
    "        idf[word] = math.log(N/count)\n",
    "    return idf\n",
    "\n",
    "def compute_tfidf(document, idf):\n",
    "    tfidf = {}\n",
    "    tf = compute_tf(document)\n",
    "    for word, tf_value in tf.items():\n",
    "        tfidf[word] = tf_value * idf[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "61f50d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the lemmetised data\n",
    "tf_data = [compute_tf(doc) for doc in clean_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f57449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF df Scores:\n",
      "       bought   several  vitality    canned       dog      food   product  \\\n",
      "0    0.043478  0.043478  0.043478  0.043478  0.043478  0.043478  0.130435   \n",
      "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.111111   \n",
      "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "996  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "997  0.000000  0.000000  0.000000  0.000000  0.000000  0.040000  0.000000   \n",
      "998  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "999  0.018868  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        found      good   quality  ...  habenero      dude   mailbox  \\\n",
      "0    0.043478  0.043478  0.043478  ...  0.000000  0.000000  0.000000   \n",
      "1    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "3    0.055556  0.055556  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "995  0.000000  0.000000  0.000000  ...  0.045455  0.000000  0.000000   \n",
      "996  0.000000  0.000000  0.000000  ...  0.000000  0.035714  0.035714   \n",
      "997  0.000000  0.080000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "998  0.000000  0.000000  0.000000  ...  0.040000  0.000000  0.000000   \n",
      "999  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "     suckered      wing    squirt    jalape    ntilde         o   sorbate  \n",
      "0        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4        0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "995      0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "996      0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "997      0.00  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "998      0.04  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "999      0.00  0.018868  0.018868  0.018868  0.018868  0.018868  0.018868  \n",
      "\n",
      "[1000 rows x 5133 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for TF\n",
    "tf_df = pd.DataFrame(tf_data).fillna(0)\n",
    "print(\"TF df Scores:\")\n",
    "print(tf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "902c6b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IDF df Scores:\n",
      "        job      fork   passing     large   chicken   vickies   premium  \\\n",
      "0  4.828314  6.907755  6.907755  3.649659  3.772261  6.907755  5.298317   \n",
      "\n",
      "      among  drawback        ok  ...    equate  complely   clumped  whatever  \\\n",
      "0  6.214608  5.809143  3.816713  ...  6.907755  6.907755  6.907755  4.710531   \n",
      "\n",
      "     robust   anodyne     panel  spicyness  switzerland    safety  \n",
      "0  6.214608  6.907755  6.907755   6.214608     6.907755  6.907755  \n",
      "\n",
      "[1 rows x 5133 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute IDF - the number of columns should be the same as you calculate in tf\n",
    "idf = compute_idf(clean_lemmatized)\n",
    "idf_df = pd.DataFrame([idf]).fillna(0)\n",
    "print(\"\\nIDF df Scores:\")\n",
    "print(idf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e03de501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF df Scores:\n",
      "       bought   several  vitality    canned       dog      food   product  \\\n",
      "0    0.107693  0.158681  0.300337  0.222435  0.148315  0.080224  0.220803   \n",
      "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.188091   \n",
      "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "995  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "996  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "997  0.000000  0.000000  0.000000  0.000000  0.000000  0.073806  0.000000   \n",
      "998  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "999  0.046735  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "        found      good   quality  ...  habenero      dude   mailbox  \\\n",
      "0    0.107178  0.059927  0.118842  ...  0.000000  0.000000  0.000000   \n",
      "1    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "3    0.136950  0.076574  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "995  0.000000  0.000000  0.000000  ...  0.282482  0.000000  0.000000   \n",
      "996  0.000000  0.000000  0.000000  ...  0.000000  0.246706  0.246706   \n",
      "997  0.000000  0.110266  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "998  0.000000  0.000000  0.000000  ...  0.248584  0.000000  0.000000   \n",
      "999  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
      "\n",
      "     suckered      wing    squirt    jalape    ntilde         o   sorbate  \n",
      "0     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "2     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4     0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "995   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "996   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "997   0.00000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "998   0.27631  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "999   0.00000  0.130335  0.130335  0.130335  0.130335  0.130335  0.130335  \n",
      "\n",
      "[1000 rows x 5133 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute TF-IDF for each document\n",
    "tfidf_data = [compute_tfidf(doc, idf) for doc in clean_lemmatized]\n",
    "\n",
    "# Create DataFrame for TF-IDF\n",
    "tfidf_df = pd.DataFrame(tfidf_data).fillna(0)\n",
    "print(\"\\nTF-IDF df Scores:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af143e",
   "metadata": {},
   "source": [
    "### Model Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3963053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f011514",
   "metadata": {},
   "source": [
    "#### Lexicon-based approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be8114f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexicon-based Approach Accuracy: 0.802\n"
     ]
    }
   ],
   "source": [
    "# Download the VADER lexicon\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Assign sentiment labels based on the 'Score' column\n",
    "def assign_sentiment(score):\n",
    "    if score >= 4:\n",
    "        return 'Positive'\n",
    "    elif score <= 2:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Sentiment'] = df['Score'].apply(assign_sentiment)\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores for each review\n",
    "# Ensure that the input to sid.polarity_scores is a string\n",
    "df['Lexicon_Sentiment'] = df['Preprocessed_Text'].apply(lambda x: sid.polarity_scores(str(x))['compound'])\n",
    "\n",
    "# Map sentiment scores to labels\n",
    "def map_sentiment_label(score):\n",
    "    if score > 0:\n",
    "        return 'Positive'\n",
    "    elif score < 0:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df['Lexicon_Sentiment_Label'] = df['Lexicon_Sentiment'].apply(map_sentiment_label)\n",
    "\n",
    "# Evaluate the lexicon-based approach\n",
    "from sklearn.metrics import accuracy_score\n",
    "lexicon_accuracy = accuracy_score(df['Sentiment'], df['Lexicon_Sentiment_Label'])\n",
    "print(\"Lexicon-based Approach Accuracy:\", lexicon_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6ca9c",
   "metadata": {},
   "source": [
    "#### Machine Learning-based Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e8028ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        25\n",
      "     Neutral       0.00      0.00      0.00        11\n",
      "    Positive       0.82      1.00      0.90       164\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.27      0.33      0.30       200\n",
      "weighted avg       0.67      0.82      0.74       200\n",
      "\n",
      "SVM Accuracy: 0.835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.28      0.40        25\n",
      "     Neutral       0.00      0.00      0.00        11\n",
      "    Positive       0.85      0.98      0.91       164\n",
      "\n",
      "    accuracy                           0.83       200\n",
      "   macro avg       0.52      0.42      0.44       200\n",
      "weighted avg       0.78      0.83      0.79       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with NaN values in the Preprocessed_Text column\n",
    "df.dropna(subset=['Preprocessed_Text'], inplace=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Preprocessed_Text'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Train and evaluate Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "nb_predictions = nb_classifier.predict(X_test_tfidf)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)\n",
    "print(classification_report(y_test, nb_predictions))\n",
    "\n",
    "# Train and evaluate SVM classifier\n",
    "svm_classifier = LinearSVC()\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "svm_predictions = svm_classifier.predict(X_test_tfidf)\n",
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(classification_report(y_test, svm_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9578d8e9",
   "metadata": {},
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c1af3-4d58-4cb0-8db8-2e79b2a09a67",
   "metadata": {},
   "source": [
    "#### 1. Lexicon-Based (VADER)\n",
    "\n",
    "   Strengths:\n",
    "   \n",
    "   ~ Fast and Efficient - Lexicon-based approaches such as VADER are fast and efficient for sentiment analysis, making them suitable for real-time applications and large-scale text data.\n",
    " \n",
    "   ~ Domain Independence - Lexicon-based approaches are relatively domain-independent and do not require labeled training data, making them adaptable to different domains and languages.\n",
    "   \n",
    "   ~ Interpretability - Lexicon-based approaches provide interpretability by assigning sentiment scores to individual words, allowing users to understand the sentiment of each word in the text.\n",
    "\n",
    "   Weaknesses:\n",
    "   \n",
    "   ~ Dependency on Lexicon Quality - Lexicon-based approaches heavily rely on the quality and coverage of the sentiment lexicon, which may not capture nuances or slang expressions in the text.\n",
    "\n",
    "   ~ Limited Contextual Understanding - Lexicon-based approaches may struggle to capture the contextual nuances of sentiment, such as sarcasm, irony, or negation, leading to less accurate predictions in certain cases.\n",
    "\n",
    "   ~ Difficulty Handling Neutral Sentiments - Lexicon-based approaches may have difficulty distinguishing between neutral and ambiguous sentiments, leading to misclassifications in cases where sentiment is subtle or mixed.\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "#### 2. Naive Bayes\n",
    "\n",
    "   Strengths:\n",
    "   \n",
    "   ~ Efficiency - Naive Bayes classifiers are computationally efficient and scale well with large datasets, making them suitable for text classification tasks.\n",
    "\n",
    "   ~ Simple and Easy to Implement - Naive Bayes classifiers are simple probabilistic models that are easy to implement and understand, making them suitable for beginners.\n",
    "\n",
    "   ~ Robustness to Noise - Naive Bayes classifiers are robust to irrelevant features and noisy data, which can be advantageous in real-world scenarios.\n",
    "\n",
    "   Weaknesses:\n",
    "   \n",
    "   ~ Assumption of Independence - The \"naive\" assumption of feature independence might not hold true in many real-world text datasets, which can lead to suboptimal performance.\n",
    "\n",
    "   ~ Limited Expressiveness - Naive Bayes classifiers assume that features are conditionally independent given the class, which limits their expressiveness and may result in underfitting for complex data distributions.\n",
    "\n",
    "   \n",
    "   \n",
    "\n",
    "#### 3. Support Vector Machine (SVM)\n",
    "\n",
    "   Strengths:\n",
    "   \n",
    "   ~ High Accuracy - SVMs are known for their ability to achieve high accuracy in text classification tasks, especially when the number of features (words) is large.\n",
    "\n",
    "   ~ Effective in High-Dimensional Spaces - SVMs perform well in high-dimensional feature spaces, making them suitable for text classification where each word represents a feature.\n",
    "\n",
    "   ~ Versatility - SVMs can use different kernel functions (e.g., linear, polynomial, radial basis function) to capture complex decision boundaries, providing flexibility in modeling.\n",
    "\n",
    "   Weaknesses:\n",
    "   \n",
    "   ~ Computational Complexity - SVMs can be computationally expensive, especially for large datasets, due to the optimization required to find the optimal hyperplane.\n",
    "\n",
    "   ~ Need for Tuning - SVMs require careful selection of hyperparameters such as the regularization parameter (C) and the choice of kernel function, which may require extensive tuning.\n",
    "\n",
    "   ~ Memory Intensive - SVMs require storing support vectors in memory, which can be memory-intensive for large datasets with many features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c79c03-2ce2-4e9a-8d42-b81141a498e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
